{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244f514f-c95b-4260-95db-c0e93c94d5d9",
   "metadata": {},
   "source": [
    "# Nova Neural Network Whiteboard\n",
    "\n",
    "This code is just me experimenting with building my own neural network, none of this has been unit tested, time or memory efficiency optimized, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f802635-7000-4650-96ea-07d896933f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb48c081-4835-43b3-959c-274d8115e563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': array([[ 5.79321837],\n",
       "        [19.78262095],\n",
       "        [10.89987604]]),\n",
       " 'world': array([[-8.99983678],\n",
       "        [36.93247135],\n",
       "        [37.73708553]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define our hidden layer\n",
    "hidden_layer_length = 3\n",
    "\n",
    "\n",
    "#define a stack of words for embedding\n",
    "word_stack = {\n",
    "    \"hello\": np.random.random((hidden_layer_length, 1))*100 - 50,\n",
    "    \"world\": np.random.random((hidden_layer_length, 1))*100 - 50\n",
    "}\n",
    "\n",
    "word_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a15eb6-65ef-4f77-828c-772c7ec7b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the \"Layer\" Object to build our network off of\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, W = None, a=None, b=None):\n",
    "        self.W = W\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "# One activation function (would be used for everything except outputs\n",
    "    @staticmethod\n",
    "    def ReLU(M):\n",
    "        arr = []\n",
    "        for v in M.reshape(1, len(M))[0]:\n",
    "            arr.append(max([v, 0]))\n",
    "        return np.array(arr).reshape(len(arr), 1)\n",
    "\n",
    "# Another activation (used for defining outputs)\n",
    "    @staticmethod\n",
    "    def softmax(M):\n",
    "        exp_values = np.exp(M - np.max(M))  # for numerical stability\n",
    "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "\n",
    "#feed to next layer\n",
    "    def feedF(self):\n",
    "        return self.ReLU(np.dot(self.W, self.a)+self.b)\n",
    "\n",
    "#feed to outputs\n",
    "    def feedO(self):\n",
    "        return self.softmax(np.dot(self.W, self.a)+self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff505ce5-24ba-47c0-9a62-b85894061a29",
   "metadata": {},
   "source": [
    "Documentation on softmax and ReLU are provided below: <br>\n",
    "**[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))** <br>\n",
    "**[softmax](https://en.wikipedia.org/wiki/Softmax_function)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055d8740-1d97-42a8-8ce0-fc08ff704978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network itself\n",
    "\n",
    "class NNova:\n",
    "    def __init__(self, output_length):\n",
    "        self.e_layer = Layer()\n",
    "        self.h_layer = Layer(W = np.random.random((output_length, hidden_layer_length))*100-50,\n",
    "                             b = np.random.random((output_length, 1))*100 - 50)\n",
    "# queue up inputs for forward pass\n",
    "    def queueInputs(self, words):\n",
    "        # adds to wordstack hash map\n",
    "        for word in words:\n",
    "            if not word in word_stack.keys():\n",
    "                print(\"generating new embedding\")\n",
    "                word_stack[word] = np.random.random((hidden_layer_length, 1))*100 - 50\n",
    "        # compile into W\n",
    "        for k in word_stack.keys():\n",
    "            try:\n",
    "                if not self.e_layer.W:\n",
    "                    self.e_layer.W = word_stack[k]\n",
    "            except ValueError:\n",
    "                self.e_layer.W = np.dstack((self.e_layer.W, word_stack[k]))\n",
    "        self.e_layer.a = [1 for w in word_stack if w in words]\n",
    "        self.e_layer.b = np.random.random((hidden_layer_length, 1))*100 - 50\n",
    "        return\n",
    "#pass forward to output layer\n",
    "    def feedForward(self):\n",
    "        self.h_layer.a = self.e_layer.feedF()\n",
    "        return self.h_layer.feedO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c65e0ab-d134-4d19-bb4a-7c030b5c4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating new embedding\n"
     ]
    }
   ],
   "source": [
    "#define neural network object with parameters\n",
    "nn = NNova(2)\n",
    "\n",
    "#queue up tokens\n",
    "nn.queueInputs([\"hello\",\"new\",\"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8baec70-3afb-4d9b-b581-65325fd01523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feed forward\n",
    "model_outputs = np.hstack(nn.feedForward())\n",
    "\n",
    "# now lets say our output is...\n",
    "output_vocab = [\"yes\", \"no\"]\n",
    "\n",
    "# then in this case the model would select\n",
    "answer = output_vocab[np.argmax(model_outputs)]\n",
    "\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
