{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244f514f-c95b-4260-95db-c0e93c94d5d9",
   "metadata": {},
   "source": [
    "# Nova Model Whiteboard\n",
    "\n",
    "This code is just me experimenting with building my own neural network, none of this has been unit tested, time or memory efficiency optimized, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f802635-7000-4650-96ea-07d896933f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from neuralnetworks import fnn, embedding, attention, masking\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8b07c24-2b23-481c-9a4d-e4c9b236e270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'neuralnetworks.attention' from '/Users/joericks/Desktop/nova/model/neuralnetworks/attention.py'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e72c0fb6-8c60-413d-bb3a-de6fe4f84316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([-2.7184415 , -1.8810238 , -0.2944612 , -1.7883768 , -0.47932693,\n",
       "       -0.16724807, -1.2596335 , -0.05437664,  0.28210768,  1.3904436 ,\n",
       "        1.9699233 ,  0.3333861 ,  1.4319669 ,  1.1096019 ,  0.4681387 ,\n",
       "        0.77374244, -0.7967433 , -1.9156507 , -1.8496896 , -0.9706564 ,\n",
       "        0.13658233, -1.6557533 ,  0.0760828 ,  0.6728236 ,  1.4790019 ,\n",
       "       -0.89459425,  0.3873762 ,  1.3861843 , -1.6747906 ,  1.1440643 ,\n",
       "        0.13914542, -0.6017897 , -0.35268176,  0.5577859 , -0.954377  ,\n",
       "       -0.1888082 , -1.0528808 , -1.3557922 , -0.7080775 ,  0.04598253,\n",
       "       -0.316238  ,  0.49337313, -0.67860955,  0.42923546,  1.3831742 ,\n",
       "        1.3132617 , -0.64454746,  0.6455149 ,  0.1456787 ,  0.60504985,\n",
       "       -0.21524344, -0.9424095 ,  0.75650513,  1.7483546 ,  1.914199  ,\n",
       "        2.4723155 , -1.5166291 ,  0.37744156,  0.9279167 , -2.3732688 ,\n",
       "       -1.4005003 ,  0.5338947 ,  1.8098997 , -0.7600517 , -1.0297866 ,\n",
       "       -0.6572477 ,  0.50977767,  0.6278692 , -1.0362543 ,  0.46906176,\n",
       "        1.1453829 , -1.9159214 , -0.08811322,  1.0677437 ,  0.3332312 ,\n",
       "        1.6642147 , -0.16188407, -2.0936522 , -0.41585758, -0.7079656 ,\n",
       "       -0.5536056 , -2.8610907 , -1.023329  ,  2.1567593 ,  0.4865381 ,\n",
       "        1.4655999 ,  0.43832293,  0.71577275,  2.1849444 , -2.0063162 ,\n",
       "       -1.2979738 ,  0.90361696, -1.9637927 , -0.45641544, -0.87608993,\n",
       "        0.9252534 , -0.8398977 ,  0.00913405,  0.09259459,  1.3622048 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_layer = embedding.EmbeddingLayer(100, name = \"embedding\")\n",
    "\n",
    "e_layer(\"hello\")\n",
    "e_layer(\"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "961297ce-1f33-485f-b53b-4838643d3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_layer = embedding.EmbeddingLayer(100, name = \"other_embedding\")\n",
    "\n",
    "other_layer(\"!\")\n",
    "\n",
    "other_layer(\"=\")\n",
    "\n",
    "other_layer (\";\")\n",
    "\n",
    "new_layer = e_layer + other_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b15d7c30-fa24-4b13-b1c6-0c9a1c460e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'world': 1, '!': 2, '=': 3, ';': 4}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layer.h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff505ce5-24ba-47c0-9a62-b85894061a29",
   "metadata": {},
   "source": [
    "Documentation on softmax and ReLU are provided below: <br>\n",
    "**[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))** <br>\n",
    "**[softmax](https://en.wikipedia.org/wiki/Softmax_function)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3e20f92-17ee-4f02-b732-4e38de1d5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the performer attention head\n",
    "\n",
    "sentence = ['hello', 'world']\n",
    "\n",
    "idxs = [new_layer.h[s] for s in sentence if s in new_layer.h.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6d5b4445-4b32-42f9-b82b-2c1058f00082",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "\n",
    "for i in idxs:\n",
    "    c.append(new_layer.embeddings[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "176cffd5-3f95-4920-8b0e-56413c3dbf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 100), dtype=float32, numpy=\n",
       "array([[-0.15870668, -1.16393   , -1.1427065 ,  2.107201  , -0.39197847,\n",
       "         0.25722778,  1.8280146 , -2.5940335 ,  1.1583526 , -1.192839  ,\n",
       "        -0.11416273, -1.245977  , -0.8502662 ,  3.1585834 , -0.7413887 ,\n",
       "        -0.79671735,  1.1449596 , -0.3737354 , -0.6010431 ,  1.1941677 ,\n",
       "         1.6023483 , -0.5696767 ,  2.451905  , -1.1505826 ,  1.036337  ,\n",
       "        -2.429174  ,  0.11398666,  1.4498316 ,  0.21689123,  1.8255156 ,\n",
       "        -0.5523072 ,  0.68873745,  2.6956604 ,  1.2260299 , -1.1639189 ,\n",
       "        -1.3993646 , -0.2551034 , -0.863256  ,  1.1543254 , -0.255144  ,\n",
       "        -0.4015707 , -1.1216762 , -1.287351  ,  0.09091989, -1.8956711 ,\n",
       "         0.21696211, -1.7282445 , -0.31165326, -2.5500822 , -1.132489  ,\n",
       "         2.0407522 , -1.5554726 ,  0.6076954 , -1.1714535 , -0.07987636,\n",
       "        -2.3211784 , -1.6130664 , -0.5348018 , -0.95790803,  0.09477446,\n",
       "        -1.1148055 , -1.3554062 ,  2.365865  ,  0.21499486,  1.0090005 ,\n",
       "        -0.10900049, -0.7015858 , -0.42436802,  2.0835788 , -0.38545862,\n",
       "        -0.8300567 ,  1.8960137 ,  1.6468352 , -2.4390798 , -1.5029624 ,\n",
       "        -2.5674543 , -0.99687415, -2.4384398 , -1.9322579 , -0.7537231 ,\n",
       "        -0.22065388, -1.6291875 ,  0.37460336, -1.1845357 , -0.5814836 ,\n",
       "         2.9959404 , -0.988256  , -1.7701102 ,  2.5280316 ,  1.9433633 ,\n",
       "         0.16040626, -1.5447391 ,  0.30407363, -1.4908452 ,  1.6637133 ,\n",
       "        -1.4160209 , -0.7871583 ,  2.785308  , -1.1134568 , -2.0855656 ],\n",
       "       [-0.15870668, -1.16393   , -1.1427065 ,  2.107201  , -0.39197847,\n",
       "         0.25722778,  1.8280146 , -2.5940335 ,  1.1583526 , -1.192839  ,\n",
       "        -0.11416273, -1.245977  , -0.8502662 ,  3.1585834 , -0.7413887 ,\n",
       "        -0.79671735,  1.1449596 , -0.3737354 , -0.6010431 ,  1.1941677 ,\n",
       "         1.6023483 , -0.5696767 ,  2.451905  , -1.1505826 ,  1.036337  ,\n",
       "        -2.429174  ,  0.11398666,  1.4498316 ,  0.21689123,  1.8255156 ,\n",
       "        -0.5523072 ,  0.68873745,  2.6956604 ,  1.2260299 , -1.1639189 ,\n",
       "        -1.3993646 , -0.2551034 , -0.863256  ,  1.1543254 , -0.255144  ,\n",
       "        -0.4015707 , -1.1216762 , -1.287351  ,  0.09091989, -1.8956711 ,\n",
       "         0.21696211, -1.7282445 , -0.31165326, -2.5500822 , -1.132489  ,\n",
       "         2.0407522 , -1.5554726 ,  0.6076954 , -1.1714535 , -0.07987636,\n",
       "        -2.3211784 , -1.6130664 , -0.5348018 , -0.95790803,  0.09477446,\n",
       "        -1.1148055 , -1.3554062 ,  2.365865  ,  0.21499486,  1.0090005 ,\n",
       "        -0.10900049, -0.7015858 , -0.42436802,  2.0835788 , -0.38545862,\n",
       "        -0.8300567 ,  1.8960137 ,  1.6468352 , -2.4390798 , -1.5029624 ,\n",
       "        -2.5674543 , -0.99687415, -2.4384398 , -1.9322579 , -0.7537231 ,\n",
       "        -0.22065388, -1.6291875 ,  0.37460336, -1.1845357 , -0.5814836 ,\n",
       "         2.9959404 , -0.988256  , -1.7701102 ,  2.5280316 ,  1.9433633 ,\n",
       "         0.16040626, -1.5447391 ,  0.30407363, -1.4908452 ,  1.6637133 ,\n",
       "        -1.4160209 , -0.7871583 ,  2.785308  , -1.1134568 , -2.0855656 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = tf.constant(c)\n",
    "\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a5be6ea0-f836-4baa-b8a0-4c007f0fe88f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'masking' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(attention)\n\u001b[0;32m----> 3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(\u001b[43mmasking\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'masking' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(attention)\n",
    "\n",
    "importlib.reload(masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f46dd3ad-d675-43a9-8254-bfbd0857ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]], shape=(100, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 19:14:50.702480: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [2,5,1,20] vs. [100,100]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [2,5,1,20] vs. [100,100] [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m attention_mech \u001b[38;5;241m=\u001b[39m attention\u001b[38;5;241m.\u001b[39mPerformerAttention(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mattention_mech\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/nova/model/neuralnetworks/attention.py:65\u001b[0m, in \u001b[0;36mPerformerAttention.__call__\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     62\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...nd,...de,...n->...ne\u001b[39m\u001b[38;5;124m'\u001b[39m, q_prime, kv, z)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# apply mask\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[43mmasking\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookahead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(attention_output, (batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim))\n\u001b[1;32m     68\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtensordot(attention_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense, axes\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/Desktop/nova/model/neuralnetworks/masking.py:17\u001b[0m, in \u001b[0;36mmasked_attention\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Add the mask to the scaled tensor.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     scaled_attention_logits \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)  \u001b[38;5;66;03m# Apply mask: set future positions to -inf\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Softmax is normalized on the last axis (seq_len_k) so that the scores\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# add up to 1.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(scaled_attention_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_heads, seq_len_q, seq_len_k)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [2,5,1,20] vs. [100,100] [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "attention_mech = attention.PerformerAttention(100, 5, 2)\n",
    "\n",
    "attention_mech(sentence_embeddings, sentence_embeddings, sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd52339-8dc1-495f-a3fe-48dbbc3db944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
