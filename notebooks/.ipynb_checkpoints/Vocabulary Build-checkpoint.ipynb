{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6343a1e4-d3c3-41ff-ab91-7fca2eb975df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 21:45:15.077524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import ast\n",
    "from nova_py import TACO\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732f13b-5a07-4c77-bdb0-b61e9578488a",
   "metadata": {},
   "source": [
    "# Overall Vocabulary build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f64783-c412-4f90-9837-ba3fde7649ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.taco = {\n",
    "            \"patterns\": {\n",
    "                \"quote_search\": r'\"[^\"]*\"|[^\"\\s]+',\n",
    "                \"word_match\": r'\\w+|[^\\w\\s]',\n",
    "                \"quote_split\": r'\\\"+|.+(?<!\\\")'\n",
    "            },\n",
    "            \"splits\": [\"\\\"\", \"'\"],\n",
    "            \"tokens\": {}\n",
    "        }\n",
    "        self.encoding = {\n",
    "            \"tags\": {\n",
    "                0: \"~relation~\",\n",
    "                1: \"~pad~\",\n",
    "                2: \"~var~\",\n",
    "                3: \"~value~\",\n",
    "                4: \"~func~\",\n",
    "                5: \"~break~\",\n",
    "                6: \"~container~\",\n",
    "                7: \"~definition~\",\n",
    "                8: \"~brelation~\",\n",
    "                9: \"~connector~\"\n",
    "            },\n",
    "            \"map\": {\n",
    "                0: [],\n",
    "                1: [],\n",
    "                2: [],\n",
    "                3: [],\n",
    "                4: [],\n",
    "                5: [],\n",
    "                6: [],\n",
    "                7: [],\n",
    "                8: [],\n",
    "                9: []\n",
    "            }\n",
    "        }\n",
    "        self.performer = {\n",
    "            \"in_tokens\": {},\n",
    "            \"out_tokens: {}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b9385-eaac-4000-88ff-a46ecd6d3274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Main (Output vocabulary for performer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ee37c4-a38b-4187-8f60-0e4daa4b0e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b6a86d088440c6b9d777e3e4b8b4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"codeparrot/codeparrot-clean\"\n",
    "\n",
    "vocab_set = datasets.load_dataset(dataset_name, 'default', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0df1bbf6-1f91-44b0-804c-0e40ada8fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_text = [i['content'] for i in vocab_set.take(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db16b77c-2790-4202-8cb7-9bf5d3584030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n",
      "Escaped\n"
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "for script in vocab_text:\n",
    "    try:\n",
    "        trees.append(ast.dump(ast.parse(script)))\n",
    "    except:\n",
    "        print(\"Escaped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b51e65f-e8d3-4990-bc4f-ee27ed72e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "for tree in trees:\n",
    "    split = re.findall(r\"(?sx)(?:\\\"\\\"\\\"(?:\\\\.|(?!\\\"\\\"\\\").)*?\\\"\\\"\\\"|'''(?:\\\\.|(?!''').)*?'''|\\\"(?:(?:[^\\\"\\\\]|\\\\.)*)\\\"|'(?:(?:[^'\\\\]|\\\\.)*)'|[^\\\"'\\d]+)\", tree)\n",
    "    splits.append(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "09e69f25-b956-4c1b-b9e8-9852d5b88b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for split in splits:\n",
    "    for string in split:\n",
    "        num_quotes = len(re.findall(r\"[\\\"\\']\", string))\n",
    "        if not num_quotes > 1:\n",
    "            tokens.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d623382d-35d2-40c6-9f6e-0ee32be29d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pattern = r'\\w+|\\W'\n",
    "\n",
    "clean_tokens = []\n",
    "for t in tokens:\n",
    "    for t2 in re.findall(cleaning_pattern, t):\n",
    "        clean_tokens.append(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f628bde4-7b53-4c16-b1e8-5d8dddf0b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, _ = tf.unique(tf.constant(clean_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31dc3ee7-b9bc-462c-b292-ee3639c850e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(157,), dtype=string, numpy=\n",
       "array([b'Module', b'(', b'body', b'=', b'[', b'ImportFrom', b'module',\n",
       "       b',', b' ', b'names', b'alias', b'name', b')', b']', b'level',\n",
       "       b'Import', b'ClassDef', b'bases', b'Name', b'id', b'ctx', b'Load',\n",
       "       b'keywords', b'FunctionDef', b'args', b'arguments', b'posonlyargs',\n",
       "       b'arg', b'kwonlyargs', b'kw_defaults', b'defaults', b'Expr',\n",
       "       b'value', b'Call', b'func', b'Attribute', b'Constant', b'attr',\n",
       "       b'decorator_list', b'While', b'test', b'True', b'kind', b'b',\n",
       "       b'keyword', b'YieldFrom', b'orelse', b'If', b'Compare', b'left',\n",
       "       b'ops', b'Eq', b'comparators', b'Assign', b'targets', b'Store',\n",
       "       b'False', b'type_ignores', b'vararg', b'kwarg', b'None', b'IfExp',\n",
       "       b'Subscript', b'slice', b'Return', b'AugAssign', b'target', b'op',\n",
       "       b'Add', b'Tuple', b'elts', b'List', b'IsNot', b'ListComp', b'elt',\n",
       "       b'generators', b'comprehension', b'iter', b'ifs', b'UnaryOp',\n",
       "       b'Not', b'operand', b'is_async', b'NotIn', b'For', b'Assert',\n",
       "       b'msg', b'BinOp', b'Mod', b'right', b'In', b'GeneratorExp',\n",
       "       b'asname', b'Starred', b'Raise', b'exc', b'Is', b'Gt', b'Slice',\n",
       "       b'upper', b'Pow', b'FloorDiv', b'Lt', b'Yield', b'Pass', b'Dict',\n",
       "       b'keys', b'values', b'BoolOp', b'And', b'Continue', b'Try',\n",
       "       b'handlers', b'ExceptHandler', b'type', b'finalbody', b'Or',\n",
       "       b'Sub', b'LtE', b'lower', b'GtE', b'With', b'items', b'withitem',\n",
       "       b'context_expr', b'Break', b'NotEq', b'optional_vars', b'USub',\n",
       "       b'LShift', b'Mult', b'.', b'Div', b'Lambda', b'Delete', b'Del',\n",
       "       b'DictComp', b'key', b'step', b'JoinedStr', b'FormattedValue',\n",
       "       b'conversion', b'-', b'AsyncFunctionDef', b'format_spec',\n",
       "       b'Global', b'e', b'Set', b'Await', b'annotation', b'returns',\n",
       "       b'RShift', b'BitOr', b'cause', b'BitAnd', b'+', b'SetComp'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9fd8b0e-d868-4038-894c-6990c7271846",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tokens, _ = tf.unique(tf.reshape(tokens, [-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68953911-9666-42d8-9734-2ff14313f823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7104,), dtype=string, numpy=\n",
       "array([b'Module(body=[ImportFrom(module=', b', names=[alias(name=',\n",
       "       b'), alias(name=', ...,\n",
       "       b', ctx=Load()), args=[ListComp(elt=Tuple(elts=[Name(id=',\n",
       "       b', ctx=Load()), ctx=Load())], ctx=Load()), generators=[comprehension(target=Name(id=',\n",
       "       b', ctx=Load())], keywords=[])), body=[Return(value=Constant(value=False))], orelse=[]), Return(value=Compare(left=Call(func=Attribute(value=Name(id='],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9af4e10f-4bfd-4e91-9338-186d6783db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dump = [w.decode('utf-8') for w in vocabulary.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b080c09c-7c5f-4c8d-8fe9-aabb7b07f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.txt\", \"w\") as f:\n",
    "    for w in text_dump:\n",
    "        f.write(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02ff84-5a74-4601-b756-a05defbf41a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b520ebd1-d2db-4523-81ff-6952f0221be0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Regex Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f2ae14b6-3967-4ba9-a827-b474b5cb6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(string):\n",
    "    if string == \"\":\n",
    "        return\n",
    "    quote_match = r'\"[^\"]*\"|[^\"\\s]+'\n",
    "    splits = re.findall(quote_match, string)\n",
    "    custom_match = r'\\w+|[^\\w\\s]'\n",
    "    quote_split = r'\\\"+|.+(?<!\\\")'\n",
    "    tokens = []\n",
    "    for i in splits:\n",
    "        if \"\\\"\" in i:\n",
    "            arr = re.findall(quote_split, i)\n",
    "            tokens += arr\n",
    "        else:\n",
    "            arr = re.findall(custom_match, i)\n",
    "            tokens += arr\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0009d138-4b17-4f3f-8f4a-82d46fd907fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"hello =\\\"good world!\\\" goodbye = 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f8c3fcd9-5806-4bba-9490-a4ec05c6baa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '=', '\"', 'good world!', '\"', 'goodbye', '=', '0']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04325a7e-cd93-4d11-93b2-8e73b593736c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
